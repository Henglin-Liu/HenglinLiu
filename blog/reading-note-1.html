<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Deep Learning Optimization Algorithms Notes - Qunzhong Wang</title>
  <meta name="author" content="Qunzhong Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="../images/profile2.jpeg" type="image/x-icon">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@500&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: Verdana, Kaiti SC, Geneva, sans-serif;
      margin: 0;
      background-color: #f4f4f9;
      color: #333;
      line-height: 1.8;
    }

    .top-nav {
      background-color: #0d254c;
      padding: 15px 5%;
      display: flex;
      justify-content: space-between;
      align-items: center;
      position: sticky; 
      top: 0;
      z-index: 1000;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
    }

    .nav-name {
      font-size: 1.8em;
      font-weight: bold;
      color: #fff;
      text-decoration: none;
      font-family: 'Montserrat', sans-serif;
    }

    .nav-links a {
      color: #fff;
      text-decoration: none;
      margin-left: 25px;
      font-size: 1.1em;
    }

    .nav-links a:hover {
      color: #a9cce3;
    }

    .main-container {
      width: 100%;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      box-sizing: border-box;
    }

    .content-card {
      background-color: #fff;
      border-radius: 12px;
      padding: 40px 60px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.08);
    }

    .back-link {
      color: #0056b3;
      text-decoration: none;
      font-size: 0.95em;
      margin-bottom: 20px;
      display: inline-block;
    }

    .back-link:hover {
      text-decoration: underline;
    }

    h1 {
      font-size: 2.2em;
      color: #0d254c;
      margin-bottom: 10px;
      border-bottom: 3px solid #0d254c;
      padding-bottom: 15px;
    }

    .meta {
      color: #888;
      font-size: 0.95em;
      margin-bottom: 30px;
    }

    .tags {
      margin: 20px 0;
    }

    .tags span {
      display: inline-block;
      background-color: #0d254c;
      color: white;
      padding: 5px 12px;
      border-radius: 3px;
      font-size: 0.9em;
      margin-right: 8px;
    }

    h2 {
      font-size: 1.6em;
      color: #0d254c;
      margin-top: 35px;
      margin-bottom: 15px;
      border-left: 4px solid #0d254c;
      padding-left: 15px;
    }

    h3 {
      font-size: 1.3em;
      color: #333;
      margin-top: 25px;
      margin-bottom: 12px;
    }

    p {
      margin-bottom: 15px;
      text-align: justify;
    }

    code {
      background-color: #f5f5f5;
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
      color: #d63384;
    }

    pre {
      background-color: #f5f5f5;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
      border-left: 4px solid #0d254c;
    }

    pre code {
      background-color: transparent;
      padding: 0;
      color: #333;
    }

    ul, ol {
      margin: 15px 0;
      padding-left: 30px;
    }

    li {
      margin-bottom: 8px;
    }

    @media (max-width: 768px) {
      .content-card {
        padding: 25px 20px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
  </style>
</head>

<body>
  <header class="top-nav">
    <a href="../index.html" class="nav-name">Qunzhong Wang</a>
    <nav class="nav-links">
      <a href="../blog.html">← Back to Blog</a>
    </nav>
  </header>

  <div class="main-container">
    <section class="content-card">
      <a href="../blog.html" class="back-link">← Back to Blog List</a>
      
      <h1>Deep Learning Optimization Algorithms Notes</h1>
      
      <div class="meta">
        Published: January 2025 | Author: Qunzhong Wang
      </div>

      <div class="tags">
        <span>Deep Learning</span>
        <span>Optimization</span>
        <span>Reading Notes</span>
      </div>

      <h2>1. Introduction</h2>
      <p>
        Optimization algorithms are at the core of deep learning training. This article summarizes the principles, characteristics, and use cases of common optimization algorithms.
      </p>

      <h2>2. Gradient Descent</h2>
      <p>
        The most basic optimization algorithm, which updates parameters along the negative gradient direction by computing the gradient of the loss function with respect to parameters:
      </p>
      <pre><code>θ = θ - η · ∇L(θ)</code></pre>
      <p>
        where <code>η</code> is the learning rate and <code>∇L(θ)</code> is the gradient of the loss function.
      </p>

      <h3>2.1 Three Variants</h3>
      <ul>
        <li><strong>Batch GD:</strong> Uses all training data to compute gradient, high computational cost but stable convergence</li>
        <li><strong>Stochastic GD (SGD):</strong> Uses a single sample each time, fast but with high variance</li>
        <li><strong>Mini-batch GD:</strong> Uses small batches of samples, balancing speed and stability</li>
      </ul>

      <h2>3. Momentum</h2>
      <p>
        Accelerates convergence and reduces oscillation by introducing a momentum term that accumulates historical gradient information:
      </p>
      <pre><code>v = β · v + (1-β) · ∇L(θ)
θ = θ - η · v</code></pre>
      <p>
        The commonly used momentum coefficient <code>β</code> is typically set to 0.9.
      </p>

      <h2>4. Adam Optimizer</h2>
      <p>
        Combining the advantages of Momentum and RMSProp, it is one of the most popular optimization algorithms. It maintains both first-order moment estimates (mean) and second-order moment estimates (variance) of gradients.
      </p>

      <h3>4.1 Core Formula</h3>
      <pre><code>m = β₁ · m + (1-β₁) · ∇L(θ)
v = β₂ · v + (1-β₂) · (∇L(θ))²
m̂ = m / (1-β₁ᵗ)
v̂ = v / (1-β₂ᵗ)
θ = θ - η · m̂ / (√v̂ + ε)</code></pre>

      <h3>4.2 Hyperparameter Settings</h3>
      <ul>
        <li>Learning rate η: commonly 0.001 or 0.0001</li>
        <li>β₁: typically set to 0.9</li>
        <li>β₂: typically set to 0.999</li>
        <li>ε: typically set to 1e-8</li>
      </ul>

      <h2>5. AdamW</h2>
      <p>
        An improved version of Adam that correctly implements weight decay. It performs excellently in models like Transformers.
      </p>
      <p>
        The main difference is decoupling weight decay from gradient computation:
      </p>
      <pre><code>θ = θ - η · (m̂ / (√v̂ + ε) + λ · θ)</code></pre>

      <h2>6. Practical Recommendations</h2>
      <ol>
        <li><strong>Prefer Adam/AdamW:</strong> Good performance on most tasks</li>
        <li><strong>Learning Rate Scheduling:</strong> Use learning rate decay strategies (e.g., cosine annealing)</li>
        <li><strong>Gradient Clipping:</strong> Prevent gradient explosion, especially in RNN/Transformer</li>
        <li><strong>Warmup:</strong> Use smaller learning rate at the beginning of training, gradually increasing</li>
      </ol>

      <h2>7. Summary</h2>
      <p>
        Choosing the right optimization algorithm and hyperparameters is crucial for model training. In practice, it is recommended:
      </p>
      <ul>
        <li>Computer Vision tasks: SGD with Momentum or AdamW</li>
        <li>NLP tasks: Adam or AdamW</li>
        <li>Reinforcement Learning: Adam</li>
      </ul>

      <h2>References</h2>
      <ul>
        <li>Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.</li>
        <li>Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization.</li>
      </ul>

    </section>
  </div>

  <footer style="text-align:center;margin-top:20px; margin-bottom: 30px; color: #888;">
    <p>&copy; 2025 Qunzhong Wang. All rights reserved.</p>
  </footer>

</body>
</html>
